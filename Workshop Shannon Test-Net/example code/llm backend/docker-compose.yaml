version: '3'
services:
  vllm-openai-chat:
    container_name: vllm-openai-chat
    image: vllm/vllm-openai:v0.7.1
    volumes:
      - ${MODELS_PATH}:/root/.cache/huggingface/hub/
    environment:
      - HF_TOKEN=${HF_TOKEN}
    entrypoint: ["python3",
      "-m",
      "vllm.entrypoints.openai.api_server",
      "--model",
      "${CHAT_MODEL_NAME}",
      "--max-model-len",
      "${MAX_MODEL_LEN_CHAT}",
      "--served-model-name",
      "${SERVED_MODEL_NAME}",
      "--trust-remote-code",
      ]
    ports:
     - "9900:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]